"""
A script to merge multiple Rank JSON trace files generated by the PyTorch Profiler 
for Distributed Data Parallel (DDP) training.

It assigns a unique Process ID (pid) to each Rank and adds necessary metadata,
to enable multi-Rank visualization and analysis in Perfetto UI 
(https://ui.perfetto.dev/) or Chrome/Edge Trace Viewer.
"""

import json
import os
import glob
import argparse
from pathlib import Path

def merge_traces(input_dir, output_file):
    """
    Find all JSON files in the input directory,
    assign unique PIDs based on their Rank IDs,
    adjust event PIDs accordingly,
    add metadata events for Rank names,
    and merge them into a single trace file.

    Args:
        input_dir (str): The directory containing all profiler JSON files.
        output_file (str): The name of the merged output file (e.g., merged_trace.json).
    """
    
    # Find all JSON files in the input directory
    trace_files = glob.glob(os.path.join(input_dir, '*.json'))

    # Print number of files found and start processing
    print(f"Found {len(trace_files)} JSON files. Starting processing...")

    all_events = []
    final_display_unit = "ms"
    base_time_sec = None

    processed_ranks = set()

    for file_path in trace_files:
        # Print current file being read
        print(f"--- Reading file: {os.path.basename(file_path)} ---")
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            # Get Rank ID from distributedInfo
            distributed_info = data.get('distributedInfo')
            if not distributed_info or 'rank' not in distributed_info:
                # Warning if essential fields are missing
                print(f"Warning: File {file_path} lacks 'distributedInfo' or 'rank' field. Skipping this file.")
                continue
            
            rank_id = distributed_info.get('rank')
            
            # Validate rank_id is non-negative
            if rank_id is None or rank_id < 0:
                print(f"Warning: Invalid rank_id ({rank_id}) in file {file_path}. Skipping this file.")
                continue
            
            if rank_id in processed_ranks:
                # Warning if a Rank ID is processed more than once
                print(f"Warning: Rank {rank_id} seems duplicated. Skipping this file.")
                continue
            
            processed_ranks.add(rank_id)
            
            # Allocate a unique PID for this Rank
            # We usually use Rank ID + 1 as the unique PID
            # (pid 0 is generally reserved for system)
            unique_pid = rank_id + 1 
            
            print(f"  Identified Rank {rank_id}. Allocated PID: {unique_pid}")
            
            # Get events and adjust PIDs
            events = data.get('traceEvents', [])
            if not events:
                print(f"  Warning: Rank {rank_id} file contains no 'traceEvents'. Skipping this file.")
                continue

            print(f"  Processing {len(events)} events...")
            for event in events:
                # Key step: overwrite original pid with our allocated unique pid
                event['pid'] = unique_pid
                all_events.append(event)
            
            # --- 4. Add Rank Name Metadata (M Event) ---
            # This makes Perfetto UI display "Rank 0", "Rank 1" on the left panel
            meta_event = {
                "name": "process_name",  # Standard metadata event name for Process Name
                "ph": "M",               # 'M' for Metadata
                "pid": unique_pid,       # Linked to our new PID
                "tid": 0,                # Thread ID (0 is common for process metadata)
                "args": {
                    "name": f"Rank {rank_id}" # The name displayed in the UI
                }
            }
            all_events.append(meta_event)
            
            # --- 5. Unify Metadata ---
            # Ensure the time unit is consistent across files
            final_display_unit = data.get("displayTimeUnit", final_display_unit)
            # Take the base timestamp from the first file processed
            if base_time_sec is None:
                base_time_sec = data.get("baseTimeInSeconds")

        except json.JSONDecodeError:
            print(f"Error: File {file_path} is not a valid JSON. Skipping.")
        except Exception as e:
            print(f"Error processing file {file_path}: {e}")
    
    # --- 6. Write the final merged file ---
    if not all_events:
        print("Error: No events were merged. Output file not generated.")
        return

    final_trace = {
        "traceEvents": all_events,
        "displayTimeUnit": final_display_unit
    }
    
    # Add the unified base timestamp if available
    if base_time_sec is not None:
        final_trace["baseTimeInSeconds"] = base_time_sec

    try:
        with open(output_file, 'w') as f:
            json.dump(final_trace, f, indent=2)  # Pretty-printed JSON for readability
            
        print("\n" + "="*30)
        print(f" Merge successful! Processed {len(processed_ranks)} Ranks.")
        print(f" Output file saved to: {output_file}")
        print(" You can now upload this file to https://ui.perfetto.dev/ for visualization.")
        print("="*30)
        
    except IOError as e:
        print(f"Error: Could not write to output file {output_file}. Error: {e}")

# --- Script Execution Entry Point ---
if __name__ == "__main__":
    # Initialize Argument Parser with a description
    parser = argparse.ArgumentParser(description="Merges Profiler JSON trace files.")
    
    parser.add_argument(
        "-i", "--input_dir", 
        type=str, 
        required=True,
        help="Directory containing profiler .json files."
    )
    
    # Output file argument (no default value here, as it's calculated later)
    parser.add_argument(
        "-o", "--output_file", 
        type=str, 
        help="Merged JSON output filename. If only a filename is specified, it is saved to the parent directory of the input path."
    )
    
    args = parser.parse_args()
    
    # --- [Key Path Calculation Logic Starts] ---
    
    # Check if the input directory exists
    input_path = Path(args.input_dir)
    if not input_path.is_dir():
        print(f"Error: Input directory '{args.input_dir}' does not exist.")
        print("Please use the -i argument to specify the correct directory.")
        exit(1) # Exit program with error code
    
    # Determine the final output path
    
    # 1. Get the parent directory of the input as the default base for output
    parent_dir = input_path.parent
    
    # 2. Determine the output filename
    if args.output_file:
        output_path = Path(args.output_file)
        # If the user specified a filename without a directory, place it in the parent_dir
        if output_path.parent == Path('.'):
             final_output_path = parent_dir / output_path
        else:
             # If the user specified a full path (e.g., /tmp/data.json), use it directly
             final_output_path = output_path
    else:
        # If the user did not specify -o, create a default name based on the input directory name
        default_name = f"{input_path.name}_merged.json" 
        final_output_path = parent_dir / default_name

    # --- [Key Path Calculation Logic Ends] ---
    
    print(f"Input Directory: {args.input_dir}")
    print(f"Output File: {final_output_path}")

    # Execute the merge function, passing the final path as a string
    merge_traces(args.input_dir, str(final_output_path))